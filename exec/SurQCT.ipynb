{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy      as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================================================================================\n",
      " TensorFlow version: 2.4.1\n",
      " Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "InputFile      = '../input/O3_UMN/'\n",
    "\n",
    "\n",
    "WORKSPACE_PATH = os.environ['WORKSPACE_PATH']\n",
    "SurQCTFldr     = WORKSPACE_PATH + '/SurQCT/surqct/'\n",
    "\n",
    "print(\"\\n======================================================================================================================================\")\n",
    "print(\" TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\" Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SurQCT]: Loading Modules and Functions ...\n",
      "[SurQCT]:   Calling SurQCT with Input File =  ../input/O3_UMN_KExcit/\n",
      "\n",
      "[SurQCT]: Keep Loading Modules and Functions...\n",
      "\n",
      "[SurQCT]: Initializing Input ...\n",
      "\n",
      "[SurQCT]: Loading Final Modules ... \n"
     ]
    }
   ],
   "source": [
    "##==============================================================================================================\n",
    "print(\"\\n[SurQCT]: Loading Modules and Functions ...\")\n",
    "\n",
    "sys.path.insert(0, SurQCTFldr  + '/src/Reading/')\n",
    "# from Reading import read_data, read_losseshistory\n",
    "sys.path.insert(0, SurQCTFldr  + '/src/Plotting/')\n",
    "from Plotting import plot_losseshistory\n",
    "# sys.path.insert(0, SurQCTFldr  + '/src/Saving/')\n",
    "# from Saving import save_parameters, save_data\n",
    "\n",
    "\n",
    "InputFile = '../input/O3_UMN_KExcit/'\n",
    "print(\"[SurQCT]:   Calling SurQCT with Input File = \", InputFile)\n",
    "sys.path.insert(0, InputFile)\n",
    "##--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "##==============================================================================================================\n",
    "print(\"\\n[SurQCT]: Keep Loading Modules and Functions...\")\n",
    "from SurQCT_Input import inputdata\n",
    "\n",
    "print(\"\\n[SurQCT]: Initializing Input ...\")\n",
    "InputData    = inputdata(WORKSPACE_PATH, SurQCTFldr)\n",
    "##--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "##==============================================================================================================\n",
    "print(\"\\n[SurQCT]: Loading Final Modules ... \")\n",
    "\n",
    "sys.path.insert(0, SurQCTFldr  + '/src/Model/' + InputData.ApproxModel + '/')\n",
    "from Model import model\n",
    "# if (InputData.ApproxModel == 'FNN'):\n",
    "#     from Model import FNN\n",
    "\n",
    "sys.path.insert(0, SurQCTFldr  + '/src/RatesType/' + InputData.RatesType + '/')\n",
    "from RatesType import generate_data\n",
    "# Generating Data\n",
    "InputData, TrainData, TestData, AllData, ExtraData = generate_data(InputData)\n",
    "##--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "PathToFldr = InputData.PathToRunFld\n",
    "try:\n",
    "    os.makedirs(PathToFldr)\n",
    "except OSError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ProPDE]: Loading Data ... \n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.split (TFOpLambda)           [(None, 10), (None,  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "normalization (Normalization)   (None, 10)           21          tf.split[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "normalization_1 (Normalization) (None, 10)           21          tf.split[0][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "Branch1_HL1 (Dense)             (None, 32)           352         normalization[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Branch2_HL1 (Dense)             (None, 32)           352         normalization_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Branch1_HL2 (Dense)             (None, 32)           1056        Branch1_HL1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Branch2_HL2 (Dense)             (None, 32)           1056        Branch2_HL1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Branch1_OL (Dense)              (None, 1)            33          Branch1_HL2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Branch2_OL (Dense)              (None, 1)            33          Branch2_HL2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1)            0           Branch1_OL[0][0]                 \n",
      "                                                                 Branch2_OL[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 2,924\n",
      "Trainable params: 2,882\n",
      "Non-trainable params: 42\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 58ms/step - loss: 101.2699 - val_loss: 101.1247\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 101.1447 - val_loss: 101.0089\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 101.0266 - val_loss: 100.8964\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 100.9118 - val_loss: 100.7875\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 100.8052 - val_loss: 100.6819\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 100.6944 - val_loss: 100.5798\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 100.5773 - val_loss: 100.4810\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 100.5031 - val_loss: 100.3853\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 100.4105 - val_loss: 100.2924\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 100.3071 - val_loss: 100.2019\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 100.2141 - val_loss: 100.1135\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 100.1229 - val_loss: 100.0266\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 100.0451 - val_loss: 99.9407\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 99.9556 - val_loss: 99.8553\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 99.8713 - val_loss: 99.7695\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 99.7950 - val_loss: 99.6829\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 99.6922 - val_loss: 99.5944\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 99.6029 - val_loss: 99.5031\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 99.5277 - val_loss: 99.4085\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 99.4284 - val_loss: 99.3096\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 99.3315 - val_loss: 99.2056\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 99.2235 - val_loss: 99.0960\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 99.1132 - val_loss: 98.9799\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 98.9949 - val_loss: 98.8569\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 98.8728 - val_loss: 98.7267\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 98.7243 - val_loss: 98.5887\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 98.6022 - val_loss: 98.4429\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 98.4504 - val_loss: 98.2889\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 98.3001 - val_loss: 98.1266\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 98.1597 - val_loss: 97.9558\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 97.9591 - val_loss: 97.7760\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 97.7985 - val_loss: 97.5878\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 97.5978 - val_loss: 97.3906\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 97.4001 - val_loss: 97.1844\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 97.1945 - val_loss: 96.9699\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 96.9555 - val_loss: 96.7461\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 96.7453 - val_loss: 96.5134\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 96.5129 - val_loss: 96.2716\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 96.2891 - val_loss: 96.0210\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 96.0275 - val_loss: 95.7612\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 95.7617 - val_loss: 95.4925\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 95.4743 - val_loss: 95.2148\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 95.2173 - val_loss: 94.9281\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 94.9255 - val_loss: 94.6328\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 94.6231 - val_loss: 94.3286\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 94.3553 - val_loss: 94.0158\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 94.0112 - val_loss: 93.6939\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 93.7245 - val_loss: 93.3634\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 93.3745 - val_loss: 93.0241\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 93.0127 - val_loss: 92.6759\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 92.6828 - val_loss: 92.3192\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 92.3304 - val_loss: 91.9537\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 91.9580 - val_loss: 91.5794\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 91.5718 - val_loss: 91.1968\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 91.1504 - val_loss: 90.8050\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 90.7999 - val_loss: 90.4051\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 90.4118 - val_loss: 89.9969\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 89.9681 - val_loss: 89.5797\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 89.5583 - val_loss: 89.1540\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 89.1514 - val_loss: 88.7203\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 88.7112 - val_loss: 88.2782\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 88.2182 - val_loss: 87.8276\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 87.8370 - val_loss: 87.3691\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 87.3630 - val_loss: 86.9027\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 86.8578 - val_loss: 86.4278\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 86.4128 - val_loss: 85.9449\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 85.9269 - val_loss: 85.4539\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 85.4179 - val_loss: 84.9548\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 84.9154 - val_loss: 84.4478\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 84.4535 - val_loss: 83.9330\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 83.9315 - val_loss: 83.4101\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 83.3532 - val_loss: 82.8791\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 82.8290 - val_loss: 82.3403\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 82.3004 - val_loss: 81.7941\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 81.8151 - val_loss: 81.2407\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 81.1383 - val_loss: 80.6792\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 80.6092 - val_loss: 80.1104\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 80.0524 - val_loss: 79.5340\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 79.78 - 0s 6ms/step - loss: 79.5859 - val_loss: 78.9503\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 78.9532 - val_loss: 78.3582\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 78.2942 - val_loss: 77.7586\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 77.7060 - val_loss: 77.1522\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 77.1096 - val_loss: 76.5388\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 76.5355 - val_loss: 75.9182\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 75.8505 - val_loss: 75.2897\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 75.1784 - val_loss: 74.6537\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 74.4788 - val_loss: 74.0108\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 73.9971 - val_loss: 73.3617\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 73.3159 - val_loss: 72.7053\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 72.6143 - val_loss: 72.0414\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 72.0412 - val_loss: 71.3712\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 71.2765 - val_loss: 70.6936\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 70.6478 - val_loss: 70.0095\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 69.8971 - val_loss: 69.3178\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 69.3079 - val_loss: 68.6204\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 68.6104 - val_loss: 67.9159\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 67.8965 - val_loss: 67.2049\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 67.1996 - val_loss: 66.4874\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 66.4284 - val_loss: 65.7634\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 65.6215 - val_loss: 65.0323\n"
     ]
    }
   ],
   "source": [
    "### Initializing the Surrogate Model\n",
    "NN = model(InputData, TrainData)\n",
    "\n",
    "### Training the Surrogate Model\n",
    "if (InputData.TrainIntFlg >= 1):\n",
    "    History = NN.train(InputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "### Plotting the Losses History\n",
    "PathToFldr = InputData.PathToFigFld\n",
    "try:\n",
    "    os.makedirs(PathToFldr)\n",
    "except OSError as e:\n",
    "    pass\n",
    "plot_losseshistory(InputData, History)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating Test Results\n",
    "if (InputData.TestIntFlg >= 1):\n",
    "    xAll      = AllData[0]\n",
    "    yAll      = AllData[1]\n",
    "    xExtra    = ExtraData\n",
    "    ChooseVec = list(xAll.columns)\n",
    "    ChooseVec.remove('Idx_i')\n",
    "\n",
    "\n",
    "    for TTran in InputData.TTranVecTest:\n",
    "        MaskVec   = (xAll['TTran_i'] == TTran)\n",
    "        xPred     = xAll.loc[MaskVec]\n",
    "        yPred     = NN.Model.predict(xPred[ChooseVec])\n",
    "        yData     = yAll.loc[MaskVec]\n",
    "        \n",
    "\n",
    "        TrainingVar = 'log10(' + InputData.RatesType + ')'\n",
    "        NLevels     = len(yPred)\n",
    "        Indx        = np.arange(NLevels)+1\n",
    "\n",
    "        Idxs = xPred.Idx_i.unique()\n",
    "        \n",
    "        for iIdx in Idxs:\n",
    "            Idxs = xPred.Idx_i.unique()\n",
    "            kIdx = xPred.index[xPred['Idx_i'] == iIdx].tolist()\n",
    "\n",
    "            fig = plt.figure()\n",
    "            plt.xlabel('Level Index')\n",
    "            plt.ylabel('$K_i^Excit$ [$cm^3$/s]')\n",
    "            if (len(yData)>0):\n",
    "                plt.scatter(xPred['Idx_j'][kIdx], yData[TrainingVar][kIdx], c='k', marker='+', linewidths =0.5, label='Data')\n",
    "            plt.scatter(xPred['Idx_j'][kIdx], yPred[kIdx], c='r', marker='x', linewidths =0.5, label='Predicted')\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "            plt.title('Excitation Rates at T = ' + str(int(TTran)) + 'K')\n",
    "            FigPath = InputData.PathToFigFld + '/TestCase_' + str(int(TTran)) + 'K.png'\n",
    "            fig.savefig(FigPath, dpi=600)\n",
    "            plt.show()\n",
    "            #plt.close()\n",
    "\n",
    "#         ### Plotting Test Results\n",
    "#         if (InputData.PlotIntFlg >= 1):\n",
    "#             plot_prediction(InputData, xPred, yData, yPred, TTran)\n",
    "\n",
    "\n",
    "#     for TTran in InputData.TTranVecExtra:\n",
    "#         MaskVec = (xExtra['TTran_i'] == TTran)\n",
    "#         xPred   = xExtra.loc[MaskVec]\n",
    "#         yPred   = NN.Model.predict(xPred[ChooseVec])\n",
    "#         yData   = []\n",
    "\n",
    "#         ### Plotting Test Results\n",
    "#         if (InputData.PlotIntFlg >= 1):\n",
    "#             plot_prediction(InputData, xPred, yData, yPred, TTran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xPred['Idx_i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 828,\n",
       " 829,\n",
       " 830,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.36883642e+00, -4.17454492e+00, -1.85874705e-01, -3.64794962e+00,\n",
       "       -5.07458024e-01, -4.35603163e+00, -1.08842039e-05, -1.27518559e+00,\n",
       "       -1.15318906e+00, -2.98601882e+00])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
